{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Models and inspects Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# === Utility Function to Count Parameters ===\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "# === Load base ResNet and get feature size ===\n",
    "base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_features = base_model.fc.in_features\n",
    "\n",
    "# --- 1) Pretrained baseline: \"pure reuse\" (we don't train it in our project) ---\n",
    "model_pretrained = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "# Keep original 1000-class head or (if we prefer) swap to 10-class; either way:\n",
    "for p in model_pretrained.parameters():\n",
    "    p.requires_grad = False    # conceptually: 0 trainable params in our project\n",
    "\n",
    "# --- 2) Feature-extracted model (NB02): backbone frozen, head trainable ---\n",
    "model_feature = models.resnet18(weights=None)\n",
    "model_feature.fc = nn.Linear(num_features, 10)\n",
    "model_feature.load_state_dict(\n",
    "    torch.load(\"./checkpoints/resnet18_feature_extraction.pth\", weights_only=False, map_location=\"cpu\")\n",
    ")\n",
    "\n",
    "# Freeze everything first\n",
    "for p in model_feature.parameters():\n",
    "    p.requires_grad = False\n",
    "# Unfreeze only classifier head (this is what we *trained* in NB02)\n",
    "for p in model_feature.fc.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# --- 3) Fine-tuned model (NB03): layer4 + head trainable ---\n",
    "model_finetuned = models.resnet18(weights=None)\n",
    "model_finetuned.fc = nn.Linear(num_features, 10)\n",
    "model_finetuned.load_state_dict(\n",
    "    torch.load(\"./checkpoints/resnet18_finetuned.pth\", weights_only=False, map_location=\"cpu\")\n",
    ")\n",
    "\n",
    "# Freeze all layers first\n",
    "for p in model_finetuned.parameters():\n",
    "    p.requires_grad = False\n",
    "# Unfreeze only layer4 and head (this is what we *trained* in NB03)\n",
    "for p in model_finetuned.layer4.parameters():\n",
    "    p.requires_grad = True\n",
    "for p in model_finetuned.fc.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# === Inspect which parameters are trainable ===\n",
    "def print_trainable_layers(model, name):\n",
    "    trainable_layers = [n for n, p in model.named_parameters() if p.requires_grad]\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"Trainable layers: {len(trainable_layers)} out of {len(list(model.parameters()))}\")\n",
    "    print(\"Example trainable params:\",\n",
    "          trainable_layers[-5:] if trainable_layers else \"None\")\n",
    "\n",
    "print_trainable_layers(model_pretrained, \"Pretrained (ImageNet)\")\n",
    "print_trainable_layers(model_feature, \"Feature-Extracted (NB02)\")\n",
    "print_trainable_layers(model_finetuned, \"Fine-Tuned (NB03)\")\n",
    "\n",
    "# === Count parameters ===\n",
    "for name, mdl in zip(\n",
    "    [\"Pretrained\", \"Feature-Extracted\", \"Fine-Tuned\"],\n",
    "    [model_pretrained, model_feature, model_finetuned]\n",
    "):\n",
    "    total, trainable = count_params(mdl)\n",
    "    print(f\"{name:18s} | Total: {total/1e6:.2f}M | \"\n",
    "          f\"Trainable: {trainable/1e6:.4f}M | \"\n",
    "          f\"Frozen: {(total-trainable)/1e6:.4f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35112a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "load test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22671770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths (must match earlier notebooks)\n",
    "DATA_ROOT = Path(\"./data/caltech101_10\")\n",
    "MAP_PATH  = Path(\"./checkpoints/class_to_idx.pth\")\n",
    "\n",
    "assert DATA_ROOT.exists(), f\"Dataset not found at {DATA_ROOT}\"\n",
    "assert (DATA_ROOT / \"test\").exists(), f\"Test split missing at {DATA_ROOT/'test'}\"\n",
    "assert MAP_PATH.exists(), f\"class_to_idx mapping not found at {MAP_PATH}\"\n",
    "\n",
    "# Load saved mapping from NB01\n",
    "class_to_idx_saved = torch.load(MAP_PATH, weights_only=False)\n",
    "idx_to_class_saved = {v: k for k, v in class_to_idx_saved.items()}\n",
    "class_names = [idx_to_class_saved[i] for i in range(len(idx_to_class_saved))]\n",
    "num_classes = len(class_names)\n",
    "print(f\"[info] Saved classes ({num_classes}): {class_names}\")\n",
    "\n",
    "# Preprocessing (must match training notebooks)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Build test dataset/loader\n",
    "test_ds = datasets.ImageFolder(DATA_ROOT / \"test\", transform=eval_tfms)\n",
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"[info] Test set size: {len(test_ds)} images\")\n",
    "print(f\"[info] Test class_to_idx (from disk): {test_ds.class_to_idx}\")\n",
    "\n",
    "# Verify mapping consistency (order and names)\n",
    "assert set(test_ds.class_to_idx.keys()) == set(class_to_idx_saved.keys()), \\\n",
    "    \"Mismatch between saved classes and test folder classes.\"\n",
    "\n",
    "# Build ordered list according to saved mapping (0..K-1)\n",
    "ordered_from_saved = [name for name, _ in sorted(class_to_idx_saved.items(), key=lambda kv: kv[1])]\n",
    "ordered_from_disk  = [name for name, _ in sorted(test_ds.class_to_idx.items(), key=lambda kv: kv[1])]\n",
    "print(f\"[check] Order (saved): {ordered_from_saved}\")\n",
    "print(f\"[check] Order (disk) : {ordered_from_disk}\")\n",
    "\n",
    "assert ordered_from_saved == ordered_from_disk, \\\n",
    "    \"Index order differs between saved mapping and current dataset. Please align folder names or mapping.\"\n",
    "\n",
    "print(\"[ok] Class index order is consistent with saved mapping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b718295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity peak at test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d55e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Unnormalize helper for display\n",
    "def unnormalize(img, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
    "    img = img.clone()\n",
    "    for c, (m, s) in enumerate(zip(mean, std)):\n",
    "        img[c] = img[c] * s + m\n",
    "    return img.clamp(0, 1)\n",
    "\n",
    "xb, yb = next(iter(test_loader))\n",
    "k = min(8, xb.size(0))\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i in range(k):\n",
    "    img_disp = unnormalize(xb[i]).permute(1, 2, 0).numpy()\n",
    "    plt.subplot(1, k, i + 1)\n",
    "    plt.imshow(img_disp)\n",
    "    plt.title(class_names[yb[i].item()], fontsize=9)\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(\"Sample images from the test split\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b63835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# CT_Task 2 – Show 4 sample \"airplanes\" images from the test set\n",
    "\n",
    "# 1. Class names from test dataset\n",
    "class_names = test_ds.classes\n",
    "\n",
    "# 2. Class index for \"airplanes\"\n",
    "CT_airplane_class_name = \"airplanes\"\n",
    "CT_airplane_class_idx = class_names.index(CT_airplane_class_name)\n",
    "\n",
    "# 3. Collect indices of airplane images\n",
    "CT_airplane_indices = [i for i, (_, label) in enumerate(test_ds) if label == CT_airplane_class_idx]\n",
    "\n",
    "# Pick first 4 samples\n",
    "CT_airplane_indices = CT_airplane_indices[:4]\n",
    "\n",
    "# 4. Plotting\n",
    "CT_airplane_fig, CT_airplane_axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "for ax, idx in zip(CT_airplane_axes, CT_airplane_indices):\n",
    "    img, label = test_ds[idx]\n",
    "    \n",
    "    # Convert CHW → HWC\n",
    "    img_np = img.numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # Denormalization (ImageNet stats)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    img_np = std * img_np + mean\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    ax.imshow(img_np)\n",
    "    ax.set_title(class_names[label])\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the three models robustly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e2dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "CKPT = Path(\"./checkpoints\")\n",
    "CKPT_FE = CKPT / \"resnet18_feature_extraction.pth\"\n",
    "CKPT_FT = CKPT / \"resnet18_finetuned.pth\"\n",
    "CKPT_PRE = CKPT / \"resnet18_pretrained.pth\"          # optional (if you created this)\n",
    "CKPT_INIT = CKPT / \"resnet18_frozen_init.pth\"        # saved in NB01\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "def make_resnet18(num_classes: int, weights=\"IMAGENET1K_V1\"):\n",
    "    m = models.resnet18(weights=getattr(models.ResNet18_Weights, weights) if isinstance(weights, str) else weights)\n",
    "    in_f = m.fc.in_features\n",
    "    m.fc = nn.Linear(in_f, num_classes)\n",
    "    return m\n",
    "\n",
    "# --- Pretrained baseline: prefer user-saved ckpt; else NB01 init; else random head on ImageNet backbone\n",
    "if CKPT_PRE.exists():\n",
    "    model_pretrained = make_resnet18(num_classes, \"IMAGENET1K_V1\")\n",
    "    model_pretrained.load_state_dict(torch.load(CKPT_PRE, weights_only=False, map_location=\"cpu\"), strict=True)\n",
    "    baseline_name = \"pretrained.pth\"\n",
    "elif CKPT_INIT.exists():\n",
    "    model_pretrained = make_resnet18(num_classes, \"IMAGENET1K_V1\")\n",
    "    model_pretrained.load_state_dict(torch.load(CKPT_INIT, weights_only=False, map_location=\"cpu\"), strict=True)\n",
    "    baseline_name = \"frozen_init.pth\"\n",
    "else:\n",
    "    model_pretrained = make_resnet18(num_classes, \"IMAGENET1K_V1\")  # random 10-class head\n",
    "    baseline_name = \"ImageNet backbone + random 10-cls head\"\n",
    "\n",
    "model_pretrained = model_pretrained.to(device).eval()\n",
    "\n",
    "# --- Feature extraction model (NB02)\n",
    "assert CKPT_FE.exists(), f\"Missing NB02 checkpoint: {CKPT_FE}\"\n",
    "model_feature = make_resnet18(num_classes, \"IMAGENET1K_V1\")\n",
    "model_feature.load_state_dict(torch.load(CKPT_FE, weights_only=False, map_location=\"cpu\"), strict=True)\n",
    "# emulate FE inference setup (freezing not required for eval, but explicit)\n",
    "for p in model_feature.parameters(): p.requires_grad = False\n",
    "model_feature = model_feature.to(device).eval()\n",
    "\n",
    "# --- Fine-tuned model (NB03)\n",
    "assert CKPT_FT.exists(), f\"Missing NB03 checkpoint: {CKPT_FT}\"\n",
    "model_finetuned = make_resnet18(num_classes, \"IMAGENET1K_V1\")\n",
    "model_finetuned.load_state_dict(torch.load(CKPT_FT, weights_only=False, map_location=\"cpu\"), strict=True)\n",
    "model_finetuned = model_finetuned.to(device).eval()\n",
    "\n",
    "print(\"[ok] Loaded models:\")\n",
    "print(f\"  - Baseline: {baseline_name}\")\n",
    "print(f\"  - Feature-extracted: {CKPT_FE.name}\")\n",
    "print(f\"  - Fine-tuned:        {CKPT_FT.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference helper and batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_batch(model, xb):\n",
    "    logits = model(xb.to(device))\n",
    "    probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "    pred_idx = probs.argmax(axis=1)\n",
    "    conf = probs.max(axis=1)\n",
    "    return pred_idx, conf\n",
    "\n",
    "# get one batch\n",
    "xb, yb = next(iter(test_loader))\n",
    "xb_dev = xb.to(device)\n",
    "\n",
    "pred_pre, conf_pre = predict_batch(model_pretrained, xb)\n",
    "pred_fe,  conf_fe  = predict_batch(model_feature,   xb)\n",
    "pred_ft,  conf_ft  = predict_batch(model_finetuned, xb)\n",
    "\n",
    "y_true = yb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# side by side visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb400fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def unnormalize(img_tensor, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]):\n",
    "    t = img_tensor.clone()\n",
    "    for c,(m,s) in enumerate(zip(mean,std)):\n",
    "        t[c] = t[c]*s + m\n",
    "    return t.clamp(0,1)\n",
    "\n",
    "k = min(12, xb.size(0))\n",
    "cols = 3  # show 3 columns per row (we'll stack rows)\n",
    "rows = k\n",
    "\n",
    "plt.figure(figsize=(10, rows * 2.0))\n",
    "for i in range(k):\n",
    "    img = unnormalize(xb[i]).permute(1,2,0).numpy()\n",
    "    t_lbl  = class_names[y_true[i]]\n",
    "    p_pre  = class_names[pred_pre[i]]\n",
    "    p_fe   = class_names[pred_fe[i]]\n",
    "    p_ft   = class_names[pred_ft[i]]\n",
    "\n",
    "    # one row per image with 1 big panel\n",
    "    ax = plt.subplot(rows, 1, i+1)\n",
    "    ax.imshow(img)\n",
    "    title = (\n",
    "        f\"T: {t_lbl} | \"\n",
    "        f\"PRE: {p_pre} ({conf_pre[i]:.2f}) | \"\n",
    "        f\"FE: {p_fe} ({conf_fe[i]:.2f}) | \"\n",
    "        f\"FT: {p_ft} ({conf_ft[i]:.2f})\"\n",
    "    )\n",
    "    ax.set_title(title, fontsize=9)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Batch predictions — True vs Pretrained (PRE), Feature-Extracted (FE), Fine-Tuned (FT)\", fontsize=12)\n",
    "plt.tight_layout(rect=[0,0,1,0.98])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa3fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabular comparsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab908a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for i in range(k):\n",
    "    rows.append({\n",
    "        \"true\": class_names[y_true[i]],\n",
    "        \"PRE_pred\": class_names[pred_pre[i]],\n",
    "        \"PRE_conf\": f\"{conf_pre[i]:.2f}\",\n",
    "        \"FE_pred\":  class_names[pred_fe[i]],\n",
    "        \"FE_conf\":  f\"{conf_fe[i]:.2f}\",\n",
    "        \"FT_pred\":  class_names[pred_ft[i]],\n",
    "        \"FT_conf\":  f\"{conf_ft[i]:.2f}\",\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1f669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics of all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ca1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, loader, name):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb).argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "    all_preds, all_labels = np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=None, zero_division=0\n",
    "    )\n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    prec_weight, rec_weight, f1_weight, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"acc\": acc,\n",
    "        \"prec_macro\": prec_macro,\n",
    "        \"rec_macro\": rec_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weight,\n",
    "        \"cm\": cm\n",
    "    }\n",
    "\n",
    "results = []\n",
    "for name, model in [\n",
    "    (\"Pretrained\", model_pretrained),\n",
    "    (\"Feature-Extracted\", model_feature),\n",
    "    (\"Fine-Tuned\", model_finetuned)\n",
    "]:\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    res = evaluate_model(model, test_loader, name)\n",
    "    results.append(res)\n",
    "    print(f\"{name} done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cdc4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare metrics of all three tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47272bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_metrics = pd.DataFrame([{\n",
    "    \"Model\": r[\"name\"],\n",
    "    \"Accuracy\": f\"{r['acc']*100:.2f}%\",\n",
    "    \"F1 (Macro)\": f\"{r['f1_macro']*100:.2f}%\",\n",
    "    \"F1 (Weighted)\": f\"{r['f1_weighted']*100:.2f}%\"\n",
    "} for r in results])\n",
    "\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d232c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize comparsion side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad1719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b9955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clean_env)",
   "language": "python",
   "name": "clean_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
